import os
import numpy as np
import matplotlib.pyplot as plt
from keras.models import load_model, Model
from keras.layers import Dense
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam

# âœ… ç¢ºä¿ TensorFlow é‹è¡Œç’°å¢ƒç©©å®š
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

# âœ… è¨­å®šåƒæ•¸
DATASET_DIR = "data/dataset/train"  # åªåŒ…å«æ–°æ°´æœçš„çˆ¬èŸ²æ•¸æ“š
VALIDATION_DIR = "data/dataset/validation"
MODEL_SAVE_PATH = "models/fruit_classifier_new_stage1.h5"  # **ç¬¬ä¸€éšæ®µè¼¸å‡º**
TARGET_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 10  # âœ… **æ¸›å°‘ Epochï¼Œé¿å…ç ´å£èˆŠçŸ¥è­˜**
INITIAL_LR = 0.0001  # âœ… **ä¿æŒä½å­¸ç¿’ç‡ï¼Œé¿å…èˆŠé¡åˆ¥æ¬Šé‡å¤§å¹…è®Šå‹•**

# âœ… è¼‰å…¥èˆŠæ¨¡å‹ï¼ˆå·²åŒ…å«èˆŠæ°´æœ + 360 æ•¸æ“šï¼‰
base_model = load_model("models/fruit_classifier_final.h5")

# âœ… å–å¾—æ–°æ°´æœçš„é¡åˆ¥
train_datagen = ImageDataGenerator(rescale=1.0 / 255)
train_generator = train_datagen.flow_from_directory(
    DATASET_DIR,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)
new_num_classes = len(train_generator.class_indices)

# âœ… å–å¾—èˆŠé¡åˆ¥æ•¸é‡
old_num_classes = base_model.output_shape[-1]

# âœ… **æª¢æŸ¥æ˜¯å¦æœ‰æ–°é¡åˆ¥**
if new_num_classes > old_num_classes:
    print(f"ğŸ”„ æª¢æ¸¬åˆ°æ–°æ°´æœï¼Œå¾ {old_num_classes} é¡ â {new_num_classes} é¡ï¼Œæ›´æ–°åˆ†é¡å±¤ï¼")

    # âœ… **å‡çµèˆŠå±¤ï¼Œé¿å…ç ´å£èˆŠæ°´æœçš„å­¸ç¿’**
    for layer in base_model.layers:
        layer.trainable = False  # **èˆŠé¡åˆ¥çš„æ¬Šé‡ä¸å‹•**

    # âœ… **å–å‡ºèˆŠæ¨¡å‹çš„åˆ†é¡å±¤**
    x = base_model.layers[-2].output
    new_output = Dense(new_num_classes, activation="softmax", name="new_output_layer")(x)  # **ç¢ºä¿åç¨±å”¯ä¸€**
    updated_model = Model(inputs=base_model.input, outputs=new_output)

    # âœ… **åˆå§‹åŒ–åˆ†é¡å±¤æ¬Šé‡**
    old_weights = base_model.layers[-1].get_weights()
    new_weights_w = np.random.normal(size=(old_weights[0].shape[0], new_num_classes)) * 0.01
    new_weights_b = np.zeros(new_num_classes)
    new_weights_w[:, :old_num_classes] = old_weights[0]  # **ä¿ç•™èˆŠé¡åˆ¥æ¬Šé‡**
    new_weights_b[:old_num_classes] = old_weights[1]

    updated_model.layers[-1].set_weights([new_weights_w, new_weights_b])
else:
    print("âœ… æ²’æœ‰æ–°é¡åˆ¥ï¼Œç›´æ¥ä½¿ç”¨åŸæ¨¡å‹")
    updated_model = base_model

# âœ… **é‡æ–°ç·¨è­¯æ¨¡å‹**
updated_model.compile(optimizer=Adam(learning_rate=INITIAL_LR), loss="categorical_crossentropy", metrics=["accuracy"])

# âœ… è¨­å®šé©—è­‰é›†
val_datagen = ImageDataGenerator(rescale=1.0 / 255)
val_generator = val_datagen.flow_from_directory(
    VALIDATION_DIR,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

# âœ… è¨­å®šå›èª¿å‡½æ•¸
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6, verbose=1)
early_stop = EarlyStopping(monitor="val_loss", patience=5, verbose=1, restore_best_weights=True)
model_checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True, monitor="val_loss", verbose=1)

# âœ… è¨“ç·´æ¨¡å‹ï¼ˆåªè¨“ç·´æ–°æ°´æœï¼‰
history = updated_model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=EPOCHS,
    callbacks=[reduce_lr, early_stop, model_checkpoint],
)

# âœ… å„²å­˜ç¬¬ä¸€éšæ®µæ¨¡å‹
updated_model.save(MODEL_SAVE_PATH)
print(f"âœ… ç¬¬ä¸€éšæ®µæ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_SAVE_PATH}")

# âœ… **ç•«å‡ºè¨“ç·´çµæœ**
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.title("Loss")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.title("Accuracy")
plt.legend()

plt.savefig("models/training_performance_new_stage1.png")
plt.show()

print("ğŸ‰ ç¬¬ä¸€éšæ®µè¨“ç·´å®Œæˆï¼ˆåªè¨“ç·´æ–°æ°´æœçš„çˆ¬èŸ²æ•¸æ“šï¼‰ï¼")