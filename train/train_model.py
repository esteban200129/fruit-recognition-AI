import os
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Model
from keras.layers import GlobalAveragePooling2D, Dense, Dropout
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from keras.applications import MobileNetV2
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam

# **ğŸ“Œ è¨­å®šåƒæ•¸**
TRAIN_DIR = "data/dataset/train"      # è¨“ç·´æ•¸æ“šç›®éŒ„
VAL_DIR = "data/dataset/validation"   # é©—è­‰æ•¸æ“šç›®éŒ„
MODEL_SAVE_PATH = "models/fruit_classifier_stage1.h5"  # **ç¬¬ä¸€éšæ®µæ¨¡å‹å„²å­˜è·¯å¾‘**
TARGET_SIZE = (224, 224)  # å½±åƒå°ºå¯¸
BATCH_SIZE = 32
EPOCHS = 30  # è¨“ç·´å›åˆæ•¸
INITIAL_LR = 0.001  # åˆå§‹å­¸ç¿’ç‡

# **ğŸ“Œ ç¢ºä¿ models è³‡æ–™å¤¾å­˜åœ¨**
os.makedirs("models", exist_ok=True)

# **ğŸ“Œ æ¨™æº–åŒ– (rescale)**
datagen = ImageDataGenerator(rescale=1.0 / 255)  

# **ğŸ“Œ è¼‰å…¥æ•¸æ“š**
train_generator = datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

val_generator = datagen.flow_from_directory(  
    VAL_DIR,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

# **ğŸ“Œ è¨ˆç®—é¡åˆ¥æ•¸é‡**
num_classes = len(train_generator.class_indices)
print(f"\nğŸ›  ç›®å‰ Google çˆ¬èŸ²çš„æ°´æœé¡åˆ¥æ•¸é‡: {num_classes}")

# **ğŸ“Œ å»ºç«‹ MobileNetV2 æ¨¡å‹**
base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights="imagenet")
base_model.trainable = False  # **å…ˆå‡çµ MobileNetV2ï¼Œé¿å…ç ´å£é è¨“ç·´æ¬Šé‡**

# **ğŸ“Œ è‡ªè¨‚åˆ†é¡é ­**
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation="relu")(x)
x = Dropout(0.5)(x)
predictions = Dense(num_classes, activation="softmax")(x)

# **ğŸ“Œ å»ºç«‹å®Œæ•´æ¨¡å‹**
model = Model(inputs=base_model.input, outputs=predictions)

# **ğŸ“Œ ç·¨è­¯æ¨¡å‹**
model.compile(optimizer=Adam(learning_rate=INITIAL_LR), loss="categorical_crossentropy", metrics=["accuracy"])

# **ğŸ“Œ è¨­å®šå›èª¿å‡½æ•¸**
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=5, min_lr=1e-5, verbose=1)
early_stop = EarlyStopping(monitor="val_loss", patience=8, verbose=1, restore_best_weights=True)
model_checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True, monitor="val_loss", verbose=1)

# **ğŸ“Œ è¨“ç·´æ¨¡å‹**
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=EPOCHS,
    callbacks=[reduce_lr, early_stop, model_checkpoint]
)

# **ğŸ“Œ å„²å­˜æ¨¡å‹**
model.save(MODEL_SAVE_PATH)
print(f"âœ… ç¬¬ä¸€éšæ®µæ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_SAVE_PATH}")

# **ğŸ“Œ ç¹ªè£½è¨“ç·´éç¨‹**
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.title("Loss")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.title("Accuracy")
plt.legend()

plt.savefig("models/training_performance_stage1.png")
plt.show()

print("ğŸ‰ è¨“ç·´å®Œæˆï¼")